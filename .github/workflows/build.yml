name: Build Flash Attention Wheel

on:
  push:
    branches: [ main ]
    tags: [ '*' ]
  pull_request:
    branches: [ main ]

jobs:
  config:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      python_versions_list: ${{ steps.set-matrix.outputs.python_versions_list }}
    steps:
      - id: set-matrix
        run: |
          # Define Python versions here - single source of truth
          cat << 'EOF' > matrix.json
          {
            "include": [
              {"python_version": "3.10", "base_image": "nvidia/cuda:12.9.1-devel-ubuntu22.04"},
              {"python_version": "3.12", "base_image": "nvidia/cuda:12.9.1-devel-ubuntu24.04"},
              {"python_version": "3.13", "base_image": "nvidia/cuda:12.9.1-devel-ubuntu24.04"}
            ]
          }
          EOF
          echo "matrix=$(cat matrix.json | jq -c .)" >> $GITHUB_OUTPUT
          echo "python_versions_list=$(cat matrix.json | jq -c '[.include[].python_version]' | jq -r 'join(", ")')" >> $GITHUB_OUTPUT

  build:
    runs-on: ubuntu-latest
    needs: config
    strategy:
      matrix: ${{ fromJson(needs.config.outputs.matrix) }}
    permissions:
      contents: write
      packages: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v5
      with:
        submodules: recursive

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker image
      run: |
        docker build --build-arg PYTHON_VERSION=${{ matrix.python_version }} --build-arg BASE_IMAGE=${{ matrix.base_image }} --target builder -t flash-attention-builder .

    - name: Create container and extract wheel
      run: |
        CONTAINER_ID=$(docker create flash-attention-builder)
        docker cp $CONTAINER_ID:/workspace/flash-attention/dist/. ./dist/
        docker rm $CONTAINER_ID

    - name: Get build info
      run: |
        BASE_IMAGE="${{ matrix.base_image }}"
        CUDA_VERSION=$(echo $BASE_IMAGE | sed 's/.*cuda:\([^ -]*\).*/\1/')
        PYTHON_VERSION="${{ matrix.python_version }}"
        TORCH_VERSION=$(awk -F'=' '/^ARG TORCH_VERSION=/ {print $2}' Dockerfile)
        echo "CUDA_VERSION=$CUDA_VERSION" >> $GITHUB_ENV
        echo "PYTHON_VERSION=$PYTHON_VERSION" >> $GITHUB_ENV
        echo "TORCH_VERSION=$TORCH_VERSION" >> $GITHUB_ENV
        echo "COMMIT_SHA=${{ github.sha }}" >> $GITHUB_ENV
        cd flash-attention
        echo "FLASH_ATTENTION_SHA=$(git rev-parse HEAD)" >> $GITHUB_ENV

    - name: Upload wheel as artifact
      uses: actions/upload-artifact@v4
      with:
        name: flash-attention-wheel-py${{ matrix.python_version }}
        path: dist/*.whl

  release:
    runs-on: ubuntu-latest
    needs: [config, build]
    if: github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/')
    permissions:
      contents: write
      packages: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v5
      with:
        submodules: recursive

    - name: Download all wheels
      uses: actions/download-artifact@v4
      with:
        pattern: flash-attention-wheel-py*
        path: wheels
        merge-multiple: true

    - name: Get build info
      run: |
        TORCH_VERSION=$(awk -F'=' '/^ARG TORCH_VERSION=/ {print $2}' Dockerfile)
        echo "TORCH_VERSION=$TORCH_VERSION" >> $GITHUB_ENV
        cd flash-attention
        echo "FLASH_ATTENTION_SHA=$(git rev-parse HEAD)" >> $GITHUB_ENV

    - name: Create latest release for main
      if: github.ref == 'refs/heads/main'
      uses: softprops/action-gh-release@v2
      with:
        tag_name: latest
        name: ${{ github.sha }}
        body: |
          Latest wheels from main branch (commit ${{ github.sha }})

          **Build Info:**
          - CUDA Version: 12.9.1
          - Torch Version: ${{ env.TORCH_VERSION }}
          - Supported Python Versions: ${{ needs.config.outputs.python_versions_list }}
          - Docker Image SHA: ${{ github.sha }}
          - Flash Attention Git Hash: ${{ env.FLASH_ATTENTION_SHA }}
        files: wheels/*.whl
        draft: false
        prerelease: true

    - name: Create release for tag
      if: startsWith(github.ref, 'refs/tags/')
      uses: softprops/action-gh-release@v2
      with:
        tag_name: ${{ github.ref_name }}
        name: Release ${{ github.ref_name }}
        body: |
          Wheels for ${{ github.ref_name }} (commit ${{ github.sha }})

          **Build Info:**
          - CUDA Version: 12.9.1
          - Torch Version: ${{ env.TORCH_VERSION }}
          - Supported Python Versions: ${{ needs.config.outputs.python_versions_list }}
          - Docker Image SHA: ${{ github.sha }}
          - Flash Attention Git Hash: ${{ env.FLASH_ATTENTION_SHA }}
        files: wheels/*.whl
        draft: false
        prerelease: false

  test:
    runs-on: ubuntu-latest
    needs: build
    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Install uv
      run: |
        curl -L https://github.com/astral-sh/uv/releases/latest/download/uv-x86_64-unknown-linux-gnu.tar.gz | tar -xz -C /usr/local/bin --strip-components=1 uv-x86_64-unknown-linux-gnu/uv

    - name: Get build info
      run: |
        BASE_IMAGE=$(awk -F'=' '/^ARG BASE_IMAGE=/ {print $2}' Dockerfile)
        CUDA_VERSION=$(echo $BASE_IMAGE | sed 's/.*cuda:\([^ -]*\).*/\1/')
        PYTHON_VERSION=$(awk -F'=' '/^ARG PYTHON_VERSION=/ {print $2}' Dockerfile)
        TORCH_VERSION=$(awk -F'=' '/^ARG TORCH_VERSION=/ {print $2}' Dockerfile)
        echo "CUDA_VERSION=$CUDA_VERSION" >> $GITHUB_ENV
        echo "PYTHON_VERSION=$PYTHON_VERSION" >> $GITHUB_ENV
        echo "TORCH_VERSION=$TORCH_VERSION" >> $GITHUB_ENV
        echo "COMMIT_SHA=${{ github.sha }}" >> $GITHUB_ENV
        cd flash-attention
        echo "FLASH_ATTENTION_SHA=$(git rev-parse HEAD)" >> $GITHUB_ENV

    - name: Download wheel from latest release
      run: gh release download latest --pattern "*.whl" --dir .
      env:
        GH_TOKEN: ${{ github.token }}

    - name: Set up test project
      run: |
        mkdir test
        mv *.whl test/
        cd test
        cat > pyproject.toml << EOF
        [project]
        name = "test"
        version = "0.1.0"
        dependencies = ["torch==${{ env.TORCH_VERSION }}"]
        requires-python = ">=${{ env.PYTHON_VERSION }}"
        EOF
        uv add *.whl

    - name: Test import
      run: |
        cd test
        uv run python -c "import flash_attn; import inspect; print('Flash attention import successful'); print('Flash attn func signature:', inspect.signature(flash_attn.flash_attn_func))"

    - name: Check for GPU
      run: |
        if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
          echo "GPU_AVAILABLE=true" >> $GITHUB_ENV
        else
          echo "GPU_AVAILABLE=false" >> $GITHUB_ENV
        fi

    - name: Run GPU test
      if: env.GPU_AVAILABLE == 'true'
      run: |
        cd test
        uv run python -c "
        import torch
        print('Torch CUDA available:', torch.cuda.is_available())
        if torch.cuda.is_available():
          import flash_attn
          print('Flash attention GPU import successful')
          # Basic functional test
          batch_size, seqlen, nheads, headdim = 1, 64, 8, 64
          q = torch.randn(batch_size, seqlen, nheads, headdim, device='cuda')
          k = torch.randn(batch_size, seqlen, nheads, headdim, device='cuda')
          v = torch.randn(batch_size, seqlen, nheads, headdim, device='cuda')
          out = flash_attn.flash_attn_func(q, k, v, dropout_p=0.0, causal=False)
          print('Flash attention function ran successfully on random data')
          print('Output shape:', out.shape)
        else:
          print('CUDA not available')
        "