name: Build Flash Attention Wheel

on:
  push:
    branches: [ main ]
    tags: [ '*' ]
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      packages: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v5
      with:
        submodules: recursive

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker image
      run: |
        docker build --target builder -t flash-attention-builder .

    - name: Create container and extract wheel
      run: |
        CONTAINER_ID=$(docker create flash-attention-builder)
        docker cp $CONTAINER_ID:/workspace/flash-attention/dist/. ./dist/
        docker rm $CONTAINER_ID

    - name: Get build info
      run: |
        BASE_IMAGE=$(awk -F'=' '/^ARG BASE_IMAGE=/ {print $2}' Dockerfile)
        CUDA_VERSION=$(echo $BASE_IMAGE | sed 's/.*cuda:\([^ -]*\).*/\1/')
        PYTHON_VERSION=$(awk -F'=' '/^ARG PYTHON_VERSION=/ {print $2}' Dockerfile)
        TORCH_VERSION=$(awk -F'=' '/^ARG TORCH_VERSION=/ {print $2}' Dockerfile)
        echo "CUDA_VERSION=$CUDA_VERSION" >> $GITHUB_ENV
        echo "PYTHON_VERSION=$PYTHON_VERSION" >> $GITHUB_ENV
        echo "TORCH_VERSION=$TORCH_VERSION" >> $GITHUB_ENV
        echo "COMMIT_SHA=${{ github.sha }}" >> $GITHUB_ENV
        cd flash-attention
        echo "FLASH_ATTENTION_SHA=$(git rev-parse HEAD)" >> $GITHUB_ENV

    - name: Upload wheel as artifact
      uses: actions/upload-artifact@v4
      with:
        name: flash-attention-wheel
        path: dist/*.whl

    - name: Create latest release for main
      if: github.ref == 'refs/heads/main'
      uses: softprops/action-gh-release@v2
      with:
        tag_name: latest
        name: ${{ github.sha }}
        body: |
          Latest wheel from main branch (commit ${{ github.sha }})

          **Build Info:**
          - CUDA Version: ${{ env.CUDA_VERSION }}
          - Python Version: ${{ env.PYTHON_VERSION }}
          - Torch Version: ${{ env.TORCH_VERSION }}
          - Docker Image SHA: ${{ env.COMMIT_SHA }}
          - Flash Attention Git Hash: ${{ env.FLASH_ATTENTION_SHA }}
        files: dist/*.whl
        draft: false
        prerelease: true

    - name: Create release for tag
      if: startsWith(github.ref, 'refs/tags/')
      uses: softprops/action-gh-release@v2
      with:
        tag_name: ${{ github.ref_name }}
        name: Release ${{ github.ref_name }}
        body: |
          Wheel for ${{ github.ref_name }}

          **Build Info:**
          - CUDA Version: ${{ env.CUDA_VERSION }}
          - Python Version: ${{ env.PYTHON_VERSION }}
          - Torch Version: ${{ env.TORCH_VERSION }}
          - Docker Image SHA: ${{ env.COMMIT_SHA }}
          - Flash Attention Git Hash: ${{ env.FLASH_ATTENTION_SHA }}
        files: dist/*.whl
        draft: false
        prerelease: false

  test:
    runs-on: ubuntu-latest
    needs: build
    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Download wheel from latest release
      run: gh release download latest --pattern "*.whl" --dir .
      env:
        GH_TOKEN: ${{ github.token }}

    - name: Set up test project
      run: |
        mkdir test
        cd test
        cat > pyproject.toml << EOF
        [project]
        name = "test"
        dependencies = ["torch"]
        EOF
        uv init
        uv add ../*.whl

    - name: Test import
      run: |
        cd test
        uv run python -c "import flash_attn; print('Flash attention import successful')"
      continue-on-error: true  # Since GPU is needed for full functionality

    - name: Check for GPU
      run: |
        if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
          echo "GPU_AVAILABLE=true" >> $GITHUB_ENV
        else
          echo "GPU_AVAILABLE=false" >> $GITHUB_ENV
        fi

    - name: Run GPU test
      if: env.GPU_AVAILABLE == 'true'
      run: |
        cd test
        uv run python -c "
        import torch
        print('Torch CUDA available:', torch.cuda.is_available())
        if torch.cuda.is_available():
          import flash_attn
          print('Flash attention GPU import successful')
          # Basic functional test
          batch_size, seqlen, nheads, headdim = 1, 64, 8, 64
          q = torch.randn(batch_size, seqlen, nheads, headdim, device='cuda')
          k = torch.randn(batch_size, seqlen, nheads, headdim, device='cuda')
          v = torch.randn(batch_size, seqlen, nheads, headdim, device='cuda')
          out = flash_attn.flash_attn_func(q, k, v, dropout_p=0.0, causal=False)
          print('Flash attention function ran successfully on random data')
          print('Output shape:', out.shape)
        else:
          print('CUDA not available')
        "